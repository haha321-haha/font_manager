import matplotlib as mpl
mpl.use("Agg")
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import jieba
from collections import Counter
import matplotlib.font_manager as fm
import os

# ä¼˜å…ˆä½¿ç”¨æœ¬åœ° FontManager ä¸€è¡Œåˆå§‹åŒ–ï¼Œå¯ç”¨ emoji é»‘ç™½åå¤‡ï¼Œç¡®ä¿ä¸­æ–‡/emoji åŒæ—¶å¯è§
try:
    from font_manager import setup_matplotlib_chinese  # æœ¬ä»“åº“æä¾›
    _fm_result = setup_matplotlib_chinese(emoji_fallback=True, emoji_prefer_color=False)
    _wordcloud_font_path_from_fm = None
    if getattr(_fm_result, 'success', False) and getattr(_fm_result, 'font_used', None):
        _wordcloud_font_path_from_fm = getattr(_fm_result.font_used, 'path', None)
except Exception:
    _wordcloud_font_path_from_fm = None

# ä½œä¸ºå…œåº•ï¼šç®€å•æ‰«æç³»ç»Ÿå­—ä½“ï¼Œä¾› WordCloud ä½¿ç”¨
def get_chinese_font_path():
    candidates = [
        '/System/Library/Fonts/Hiragino Sans GB.ttc',
        '/System/Library/Fonts/PingFang.ttc',
        '/Library/Fonts/Arial Unicode.ttf',
        '/System/Library/Fonts/STHeiti Medium.ttc',
        '/System/Library/Fonts/STHeiti Light.ttc'
    ]
    for p in candidates:
        if os.path.exists(p):
            return p
    for fp in fm.findSystemFonts():
        if any(k in fp.lower() for k in ['hiragino', 'pingfang', 'heiti', 'simhei', 'msyh', 'arial unicode']):
            return fp
    return None

# æ•°æ®é¢„å¤„ç†
print("æ­£åœ¨åŠ è½½æ•°æ®...")
df = pd.read_csv('/Users/duting/Downloads/å‘½ç†é£æ°´å åœğŸ”®/fixed_twitter_data/specific_keywords_865_20250804_171510.csv')
df['created_at'] = pd.to_datetime(df['created_at'], format='%a %b %d %H:%M:%S %z %Y', errors='coerce')
print(f"æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(df)} æ¡æ¨æ–‡")

# 1. æå–é£æ°´ç›¸å…³æ¨æ–‡
fengshui_keywords = ['feng shui', 'é£æ°´', 'é¢¨æ°´', 'å®¶å±…å¸ƒå±€', 'å®¶ç›¸', 'è¿åŠ¿', 'æ‹›è´¢', 'è¾Ÿé‚ª', 'äº”è¡Œ', 'å…«å¦']
pattern = '|'.join(fengshui_keywords)
fengshui_tweets = df[df['text'].str.contains(pattern, case=False, na=False)].copy()
print(f"æ‰¾åˆ° {len(fengshui_tweets)} æ¡é£æ°´ç›¸å…³æ¨æ–‡")

# 2. ä¸»é¢˜åˆ†ç±»ä¸æ„å›¾åˆ†æ
theme_data = {
    'è£…é¥°': {'keywords': ['decor', 'decoration', 'è£…é¥°', 'æ“ºè¨­', 'æ‘†ä»¶', 'è²”è²…', 'æ°´æ™¶', 'crystal'],
           'intent': 'å•†ä¸šæ¨å¹¿', 'color': '#FF7F0E'},
    'å®¶å±…å¸ƒå±€': {'keywords': ['layout', 'arrangement', 'å¸ƒå±€', 'å®¶ç›¸', 'æ–¹ä½', 'æˆ·å‹', 'room'],
              'intent': 'å®ç”¨æŒ‡å—', 'color': '#1F77B4'},
    'è¿åŠ¿é¢„æµ‹': {'keywords': ['fortune', 'luck', 'é‹å‹¢', 'æ‹›è´¢', 'è´¢è¿', 'å…«å­—', 'ç”Ÿè‚–', 'prediction'],
              'intent': 'å¿ƒç†å®‰æ…°', 'color': '#2CA02C'},
    'ç†è®ºæ¢è®¨': {'keywords': ['theory', 'äº”è¡Œ', 'å…«å¦', 'é˜´é˜³', 'energy flow', 'äº‰è®®', 'philosophy'],
              'intent': 'æ–‡åŒ–ä¼ æ’­', 'color': '#D62728'}
}

# 3. ä¸»é¢˜é¢‘ç‡ç»Ÿè®¡
theme_counts = {}
for theme in theme_data:
    theme_counts[theme] = len(fengshui_tweets[fengshui_tweets['text'].str.contains(
        '|'.join(theme_data[theme]['keywords']), case=False, na=False)])

# 4. å¯è§†åŒ–è®¾ç½®
plt.style.use('default')
fig = plt.figure(figsize=(18, 20))

# ä¸»é¢˜é¢‘ç‡åˆ†å¸ƒ
plt.subplot(3, 2, 1)
theme_df = pd.DataFrame.from_dict(theme_counts, orient='index', columns=['Count'])
theme_df['Percentage'] = theme_df['Count'] / max(len(fengshui_tweets), 1) * 100
theme_df = theme_df.sort_values('Count', ascending=False)

ax = sns.barplot(x=theme_df.index, y='Count', data=theme_df,
                palette=[theme_data[t]['color'] for t in theme_df.index])
plt.title('é£æ°´ä¸»é¢˜é¢‘ç‡åˆ†å¸ƒ', fontsize=16, fontweight='bold')
plt.xlabel('ä¸»é¢˜ç±»åˆ«', fontsize=14)
plt.ylabel('æ¨æ–‡æ•°é‡', fontsize=14)
for p in ax.patches:
    ax.annotate(f'{p.get_height()}\n({p.get_height()/max(len(fengshui_tweets),1)*100:.1f}%)',
                (p.get_x() + p.get_width()/2., p.get_height()),
                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=10)

# ä¸»é¢˜äº’åŠ¨å¯¹æ¯”
plt.subplot(3, 2, 2)
interaction_data = []
for theme in theme_data:
    theme_tweets = fengshui_tweets[fengshui_tweets['text'].str.contains(
        '|'.join(theme_data[theme]['keywords']), case=False, na=False)]
    if len(theme_tweets) > 0:
        interaction_data.append({
            'Theme': theme,
            'Avg Likes': theme_tweets['like_count'].mean(),
            'Avg Retweets': theme_tweets['retweet_count'].mean(),
            'Avg Replies': theme_tweets['reply_count'].mean(),
            'Intent': theme_data[theme]['intent']
        })
interaction_df = pd.DataFrame(interaction_data)
if len(interaction_df) > 0:
    interaction_df.plot(x='Theme', y=['Avg Likes', 'Avg Retweets', 'Avg Replies'],
                       kind='bar', ax=plt.gca(), color=['#FF7F0E', '#1F77B4', '#2CA02C'])
    plt.title('å„ä¸»é¢˜å¹³å‡äº’åŠ¨é‡å¯¹æ¯”', fontsize=16, fontweight='bold')
    plt.xlabel('ä¸»é¢˜ç±»åˆ«', fontsize=14)
    plt.ylabel('å¹³å‡äº’åŠ¨é‡', fontsize=14)
    plt.xticks(rotation=45)
    plt.legend(title='äº’åŠ¨ç±»å‹')

# ä¸»é¢˜æ—¶é—´è¶‹åŠ¿
plt.subplot(3, 2, (3,4))
fengshui_tweets_with_date = fengshui_tweets.copy()
fengshui_tweets_with_date['date'] = pd.to_datetime(fengshui_tweets_with_date['created_at']).dt.date
trend_data = []
for theme in theme_data:
    theme_tweets = fengshui_tweets_with_date[fengshui_tweets_with_date['text'].str.contains(
        '|'.join(theme_data[theme]['keywords']), case=False, na=False)]
    if len(theme_tweets) > 0:
        trend = theme_tweets.groupby('date').size().reset_index(name='Count')
        trend['Theme'] = theme
        trend_data.append(trend)
if trend_data:
    trend_df = pd.concat(trend_data)
    sns.lineplot(x='date', y='Count', hue='Theme', data=trend_df,
               palette=[theme_data[t]['color'] for t in theme_data], linewidth=2.5)
    plt.title('é£æ°´ä¸»é¢˜æ—¶é—´è¶‹åŠ¿', fontsize=16, fontweight='bold')
    plt.xlabel('æ—¥æœŸ', fontsize=14)
    plt.ylabel('æ¯æ—¥æ¨æ–‡é‡', fontsize=14)
    plt.legend(title='ä¸»é¢˜ç±»åˆ«')

# ä½œè€…ç±»å‹åˆ†æ
plt.subplot(3, 2, 5)
author_data = []
for theme in theme_data:
    theme_tweets = fengshui_tweets[fengshui_tweets['text'].str.contains(
        '|'.join(theme_data[theme]['keywords']), case=False, na=False)]
    if len(theme_tweets) > 0:
        commercial = len(theme_tweets[theme_tweets['author_is_verified'] == True])
        personal = len(theme_tweets[theme_tweets['author_is_verified'] == False])
        author_data.extend([
            {'Theme': theme, 'Type': 'å•†ä¸šè´¦å·', 'Count': commercial},
            {'Theme': theme, 'Type': 'ä¸ªäººç”¨æˆ·', 'Count': personal}
        ])
author_df = pd.DataFrame(author_data)
if len(author_df) > 0:
    sns.barplot(x='Theme', y='Count', hue='Type', data=author_df,
               palette=['#9467BD', '#8C564B'])
    plt.title('å„ä¸»é¢˜ä½œè€…ç±»å‹åˆ†å¸ƒ', fontsize=16, fontweight='bold')
    plt.xlabel('ä¸»é¢˜ç±»åˆ«', fontsize=14)
    plt.ylabel('æ¨æ–‡æ•°é‡', fontsize=14)
    plt.legend(title='è´¦å·ç±»å‹')

# è¯­è¨€åˆ†å¸ƒåˆ†æ
plt.subplot(3, 2, 6)
lang_data = []
for theme in theme_data:
    theme_tweets = fengshui_tweets[fengshui_tweets['text'].str.contains(
        '|'.join(theme_data[theme]['keywords']), case=False, na=False)]
    if len(theme_tweets) > 0:
        lang_counts = theme_tweets['language'].value_counts().head(3)
        for lang, count in lang_counts.items():
            lang_data.append({'Theme': theme, 'Language': lang, 'Count': count})
lang_df = pd.DataFrame(lang_data)
if len(lang_df) > 0:
    sns.barplot(x='Theme', y='Count', hue='Language', data=lang_df,
               palette=['#E377C2', '#7F7F7F', '#BCBD22'])
    plt.title('å„ä¸»é¢˜è¯­è¨€åˆ†å¸ƒ(Top3)', fontsize=16, fontweight='bold')
    plt.xlabel('ä¸»é¢˜ç±»åˆ«', fontsize=14)
    plt.ylabel('æ¨æ–‡æ•°é‡', fontsize=14)
    plt.legend(title='è¯­è¨€')

plt.tight_layout()
plt.savefig('/Users/duting/Downloads/å‘½ç†é£æ°´å åœğŸ”®/çˆ¬è™«åˆ†æå›¾è¡¨/é£æ°´ä¸»é¢˜åˆ†æå›¾è¡¨.png', 
            dpi=300, bbox_inches='tight')
# plt.show()  # éäº¤äº’ç¯å¢ƒä¸æ˜¾ç¤º
plt.close()

# è¯äº‘åˆ†æ
def chinese_word_segmentation(text):
    """ä¸­æ–‡åˆ†è¯å¤„ç†"""
    try:
        seg_list = jieba.cut(str(text), cut_all=False)
        return ' '.join(seg_list)
    except:
        return str(text)

print("æ­£åœ¨ç”Ÿæˆè¯äº‘...")
all_text = ' '.join(fengshui_tweets['text'].astype(str))
all_text = chinese_word_segmentation(all_text)

font_path = get_chinese_font_path()
wordcloud_kwargs = dict(width=1000, height=500, background_color='white', collocations=False, max_words=100)
if font_path:
    wordcloud_kwargs['font_path'] = font_path
wordcloud = WordCloud(**wordcloud_kwargs).generate(all_text)

plt.figure(figsize=(15, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('é£æ°´ä¸»é¢˜å…³é”®è¯è¯äº‘', fontsize=16, fontweight='bold')
plt.tight_layout()
plt.savefig('/Users/duting/Downloads/å‘½ç†é£æ°´å åœğŸ”®/çˆ¬è™«åˆ†æå›¾è¡¨/é£æ°´ä¸»é¢˜è¯äº‘.png', 
            dpi=300, bbox_inches='tight')
# plt.show()
plt.close()

print("âœ… é£æ°´ä¸»é¢˜åˆ†æå®Œæˆï¼")
print(f"ğŸ“Š ç”Ÿæˆäº† {len(fengshui_tweets)} æ¡é£æ°´ç›¸å…³æ¨æ–‡çš„åˆ†æ")
print(f"ğŸ“ å›¾è¡¨å·²ä¿å­˜åˆ°: /Users/duting/Downloads/å‘½ç†é£æ°´å åœğŸ”®/çˆ¬è™«åˆ†æå›¾è¡¨/")